{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ede58a-49d0-4cbb-bac3-a6daeb5fa5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5538fbd-c755-4cd7-a7df-bf18d68e4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c2eab-2e3f-4e0e-ad09-697f4bbf403e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mempersiapkan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4e7161-4e1d-4bee-8b09-7213c42703c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/iqbal/Downloads/CHALLANGE PLATINUM/train_preprocess.tsv.txt\",encoding=\"latin1\",sep='\\t',header=None,names=[\"text\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0bf78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()#aman jaya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8adcd-4995-40dd-812c-9ab29529eb2f",
   "metadata": {},
   "source": [
    "#### Normalisasi Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f23def-bb15-4739-8ccc-8a24eb0ef859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(paragraph):\n",
    "    return paragraph.lower()\n",
    "\n",
    "def menghilangkan_tandabaca(paragraph):\n",
    "    new_paragraph = re.sub(fr'[{punctuation}]', r'', paragraph)\n",
    "    return new_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b97d9e-8515-4d88-9142-1d1fe8e01aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(paragraph):\n",
    "    paragraph = lowercasing(paragraph)\n",
    "    paragraph = menghilangkan_tandabaca(paragraph)\n",
    "    paragraph = re.sub(r\"[ ]+\",r' ',paragraph)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d6f2a5-6729-46db-9d23-385f1d7bcbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data,test_data=train_test_split(df)\n",
    "# train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\n",
    "# train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42901a27-8843-4ce1-9df7-490849a53b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(lambda x: text_normalization(x))\n",
    "# val_data['text'] = val_data['text'].apply(lambda x: text_normalization(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: text_normalization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd19ae6-00ce-4d7d-b6a3-7bb54c4b4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8250, 2)\n",
      "(2750, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "# print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb42f7e-a5c6-4fa3-a892-fe04434c7f6b",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5eba5a4-97ba-48a6-a513-65bcc5b79bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 100000\n",
    "tokenizer = Tokenizer(oov_token='<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e170e7-2cd6-47f3-89ee-54bf97d411d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552a3935-ff24-4279-b94e-469200c42709",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tf = tokenizer.texts_to_sequences(train_data['text'])\n",
    "# val_data_tf = tokenizer.texts_to_sequences(val_data['text'])\n",
    "test_data_tf = tokenizer.texts_to_sequences(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4af62d-89a3-4a8a-a8ed-6dcefa612b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_len = int(np.quantile([len(x) for x in train_data_tf], 0.9))\n",
    "train_padded = pad_sequences(sequences=train_data_tf, padding='post')\n",
    "max_len = train_padded.shape[1]\n",
    "# val_padded = pad_sequences(sequences=val_data_tf,padding='post', maxlen=max_len)\n",
    "test_padded = pad_sequences(sequences=test_data_tf,padding='post', maxlen=train_padded.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee5fb17-1ca6-4af7-b3df-5d74caadfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddd28033-7f34-4914-80d8-52b1077109c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = onehot.fit_transform(train_data[['label']]).toarray()\n",
    "# valid_labels = onehot.fit_transform(val_data[['label']]).toarray()\n",
    "test_labels = onehot.fit_transform(test_data[['label']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "674ca4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e8584-b829-438c-8b71-49a9e133a6a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare Train & Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170551ac-61e8-4740-a904-b36c904cb613",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62485190-bdaf-4464-af66-ebbf01c5563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adf1c184-3cc3-42ec-bb52-d4df4da96bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(tokenizer.index_word)\n",
    "batch_size=16\n",
    "output_dim=64\n",
    "labels_tmp=32\n",
    "# input_len = max_len\n",
    "input_len =train_padded.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6130f703-d0d5-44f3-b8c1-1c8b7f0d520d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15506"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ced3abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah unik token dalam kamus: 15506\n"
     ]
    }
   ],
   "source": [
    "# Mendapatkan jumlah unik token\n",
    "num_unique_tokens = len(tokenizer.word_index)\n",
    "print(\"Jumlah unik token dalam kamus:\", num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f22cbb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 'massal' memiliki indeks 15506, yang lebih besar dari jumlah unik token.\n"
     ]
    }
   ],
   "source": [
    "for token, index in tokenizer.word_index.items():\n",
    "    if index >= num_unique_tokens:\n",
    "        print(f\"Token '{token}' memiliki indeks {index}, yang lebih besar dari jumlah unik token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2de48ddd-77b7-4e4e-993b-5a2aba63cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/71207850/tensorflow-keras-not-utilizing-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "403c07a1-f172-4bd9-b9b8-60aa5361864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(input_dim=len(tokenizer.index_word),\n",
    "                                 output_dim=output_dim, \n",
    "                                 input_length=input_len))\n",
    "model.add(keras.layers.LSTM(128, dropout=0.2))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82cd2710-c2b9-4078-8347-52e93b11d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02e079d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index_train=int(np.floor(train_padded.shape[0]/batch_size)*batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aac809f5-1c63-459f-a41c-9de3cadec669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "438/438 [==============================] - 92s 197ms/step - loss: 0.9193 - accuracy: 0.5842 - val_loss: 0.9017 - val_accuracy: 0.5866\n",
      "Epoch 2/3\n",
      "438/438 [==============================] - 102s 232ms/step - loss: 0.9124 - accuracy: 0.5854 - val_loss: 0.9007 - val_accuracy: 0.5866\n",
      "Epoch 3/3\n",
      "438/438 [==============================] - 79s 180ms/step - loss: 0.9120 - accuracy: 0.5854 - val_loss: 0.9018 - val_accuracy: 0.5866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2782b045810>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_padded[:max_index_train], \n",
    "          y=train_labels[:max_index_train],\n",
    "          batch_size=batch_size, \n",
    "          epochs=3, \n",
    "          shuffle=True,validation_split=0.15)\n",
    "# maximum_index_training = int(np.floor(train_padded.shape[0]/batch_size)*batch_size)\n",
    "# model.fit(x=train_padded[:maximum_index_training], \n",
    "#           y=train_labels[:maximum_index_training],\n",
    "#           batch_size=batch_size, \n",
    "#           epochs=3, \n",
    "#           shuffle=False\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f91667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26e109b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2750, 91)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "800dc01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 6s 115ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.29693985, 0.09283732, 0.61022276],\n",
       "       [0.29693988, 0.09283732, 0.61022276],\n",
       "       [0.29693985, 0.09283732, 0.61022276],\n",
       "       ...,\n",
       "       [0.2969402 , 0.09283781, 0.61022204],\n",
       "       [0.29693988, 0.09283732, 0.61022276],\n",
       "       [0.29693985, 0.09283732, 0.61022276]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction=model.predict(test_padded,batch_size=64)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "618a9317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2750"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onehot.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e87408b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8279</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9014</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label\n",
       "8279  negative\n",
       "5758  negative\n",
       "5863   neutral\n",
       "9014   neutral\n",
       "2255  positive\n",
       "...        ...\n",
       "8096  positive\n",
       "2296  positive\n",
       "6243  negative\n",
       "3231  positive\n",
       "1497   neutral\n",
       "\n",
       "[2750 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5d3078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       851\n",
      "     neutral       0.00      0.00      0.00       312\n",
      "    positive       0.58      1.00      0.73      1587\n",
      "\n",
      "    accuracy                           0.58      2750\n",
      "   macro avg       0.19      0.33      0.24      2750\n",
      "weighted avg       0.33      0.58      0.42      2750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iqbal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\iqbal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\iqbal\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=test_data[[\"label\"]],y_pred=onehot.inverse_transform(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6187e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "137/137 [==============================] - 20s 134ms/step - loss: 0.7348 - accuracy: 0.6902\n",
      "Epoch 2/3\n",
      "137/137 [==============================] - 20s 144ms/step - loss: 0.3149 - accuracy: 0.8880\n",
      "Epoch 3/3\n",
      "137/137 [==============================] - 21s 151ms/step - loss: 0.1563 - accuracy: 0.9498\n",
      "35/35 [==============================] - 1s 38ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.84      0.83       688\n",
      "     neutral       0.82      0.72      0.77       240\n",
      "    positive       0.91      0.92      0.91      1272\n",
      "\n",
      "    accuracy                           0.87      2200\n",
      "   macro avg       0.85      0.83      0.84      2200\n",
      "weighted avg       0.87      0.87      0.87      2200\n",
      "\n",
      "Epoch 1/3\n",
      "137/137 [==============================] - 23s 148ms/step - loss: 0.7645 - accuracy: 0.6529\n",
      "Epoch 2/3\n",
      "137/137 [==============================] - 21s 155ms/step - loss: 0.3210 - accuracy: 0.8815\n",
      "Epoch 3/3\n",
      "137/137 [==============================] - 19s 138ms/step - loss: 0.1479 - accuracy: 0.9502\n",
      "35/35 [==============================] - 2s 41ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.83       690\n",
      "     neutral       0.89      0.70      0.78       236\n",
      "    positive       0.92      0.90      0.91      1274\n",
      "\n",
      "    accuracy                           0.87      2200\n",
      "   macro avg       0.86      0.82      0.84      2200\n",
      "weighted avg       0.87      0.87      0.87      2200\n",
      "\n",
      "Epoch 1/3\n",
      "137/137 [==============================] - 22s 148ms/step - loss: 0.7341 - accuracy: 0.6818\n",
      "Epoch 2/3\n",
      "137/137 [==============================] - 20s 148ms/step - loss: 0.3165 - accuracy: 0.8856\n",
      "Epoch 3/3\n",
      "137/137 [==============================] - 24s 173ms/step - loss: 0.1399 - accuracy: 0.9520\n",
      "35/35 [==============================] - 2s 48ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.79      0.83       720\n",
      "     neutral       0.88      0.74      0.80       217\n",
      "    positive       0.88      0.94      0.91      1263\n",
      "\n",
      "    accuracy                           0.87      2200\n",
      "   macro avg       0.87      0.83      0.85      2200\n",
      "weighted avg       0.87      0.87      0.87      2200\n",
      "\n",
      "Epoch 1/3\n",
      "137/137 [==============================] - 25s 164ms/step - loss: 0.7088 - accuracy: 0.6983\n",
      "Epoch 2/3\n",
      "137/137 [==============================] - 20s 146ms/step - loss: 0.3996 - accuracy: 0.8306\n",
      "Epoch 3/3\n",
      "137/137 [==============================] - 24s 174ms/step - loss: 0.2731 - accuracy: 0.8945\n",
      "35/35 [==============================] - 1s 39ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.87      0.82       678\n",
      "     neutral       0.82      0.44      0.57       253\n",
      "    positive       0.89      0.91      0.90      1269\n",
      "\n",
      "    accuracy                           0.85      2200\n",
      "   macro avg       0.83      0.74      0.77      2200\n",
      "weighted avg       0.85      0.85      0.84      2200\n",
      "\n",
      "Epoch 1/3\n",
      "137/137 [==============================] - 22s 149ms/step - loss: 0.7130 - accuracy: 0.6816\n",
      "Epoch 2/3\n",
      "137/137 [==============================] - 22s 163ms/step - loss: 0.3487 - accuracy: 0.8601\n",
      "Epoch 3/3\n",
      "137/137 [==============================] - 27s 195ms/step - loss: 0.1708 - accuracy: 0.9469\n",
      "35/35 [==============================] - 1s 32ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.85      0.84       660\n",
      "     neutral       0.81      0.77      0.79       202\n",
      "    positive       0.93      0.94      0.94      1338\n",
      "\n",
      "    accuracy                           0.90      2200\n",
      "   macro avg       0.86      0.85      0.86      2200\n",
      "weighted avg       0.89      0.90      0.89      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#semua berjalan dengan lanca maka tinggal copas untuk kfold\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(random_state=0,shuffle=True)\n",
    "for train,test in kfold.split(df):\n",
    "    train_data=df.loc[train]\n",
    "    test_data=df.loc[test]\n",
    "    onehot=OneHotEncoder()\n",
    "    label=onehot.fit_transform(train_data[[\"label\"]])\n",
    "    \n",
    "    #data udah ada\n",
    "    train_data['text'] = train_data['text'].apply(lambda x: text_normalization(x))\n",
    "    test_data['text'] = test_data['text'].apply(lambda x: text_normalization(x))\n",
    "    \n",
    "    tokenizer=Tokenizer(oov_token=\"UNK\")\n",
    "    tokenizer.fit_on_texts(train_data[\"text\"])\n",
    "    train_data_tf=tokenizer.texts_to_sequences(train_data[\"text\"])\n",
    "    test_data_tf=tokenizer.texts_to_sequences(test_data[\"text\"])\n",
    "    \n",
    "    train_data_pad=pad_sequences(train_data_tf,padding=\"post\")\n",
    "    test_data_pad=pad_sequences(test_data_tf,padding=\"post\",maxlen=train_data_pad.shape[1])\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(layers.Embedding(len(tokenizer.index_word),64,input_length=train_data_pad.shape[1]))\n",
    "    model.add(layers.Conv1D(128,5,activation=\"relu\"))\n",
    "    model.add(layers.GlobalMaxPool1D())\n",
    "    model.add(layers.Dense(10,activation=\"relu\"))\n",
    "    model.add(layers.Dense(3,activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'],)\n",
    "    \n",
    "    batch_size=64\n",
    "    max_index_train=int(np.floor(train_data_pad.shape[0]/batch_size)*batch_size)\n",
    "    model.fit(x=train_data_pad[:max_index_train],y=label.toarray()[:max_index_train],batch_size=64,epochs=3,shuffle=True)\n",
    "    prediction=model.predict(test_data_pad,batch_size=64)\n",
    "    prediction=onehot.inverse_transform(prediction)\n",
    "    print(classification_report(y_true=test_data[[\"label\"]],y_pred=prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd8fd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report=classification_report(y_true=test_data[[\"label\"]],y_pred=prediction,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fee0ea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 0.8403614457831325,\n",
       "  'recall': 0.8454545454545455,\n",
       "  'f1-score': 0.8429003021148036,\n",
       "  'support': 660},\n",
       " 'neutral': {'precision': 0.8082901554404145,\n",
       "  'recall': 0.7722772277227723,\n",
       "  'f1-score': 0.789873417721519,\n",
       "  'support': 202},\n",
       " 'positive': {'precision': 0.9344750558451228,\n",
       "  'recall': 0.9379671150971599,\n",
       "  'f1-score': 0.9362178291682207,\n",
       "  'support': 1338},\n",
       " 'accuracy': 0.895,\n",
       " 'macro avg': {'precision': 0.8610422190228899,\n",
       "  'recall': 0.8518996294248259,\n",
       "  'f1-score': 0.8563305163348477,\n",
       "  'support': 2200},\n",
       " 'weighted avg': {'precision': 0.8946549046984571,\n",
       "  'recall': 0.895,\n",
       "  'f1-score': 0.8947854932739076,\n",
       "  'support': 2200}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88b55822",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_report = {\n",
    "    'precision': {},\n",
    "    'recall': {},\n",
    "    'f1-score': {},\n",
    "    'support': {}\n",
    "}\n",
    "\n",
    "for class_label, metrics in class_report.items():\n",
    "    if class_label in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        continue\n",
    "    formatted_report['precision'][class_label] = metrics['precision']\n",
    "    formatted_report['recall'][class_label] = metrics['recall']\n",
    "    formatted_report['f1-score'][class_label] = metrics['f1-score']\n",
    "    formatted_report['support'][class_label] = metrics['support']\n",
    "\n",
    "# Masukkan metrik keseluruhan\n",
    "overall_metrics = class_report['macro avg']\n",
    "formatted_report['precision']['macro avg'] = overall_metrics['precision']\n",
    "formatted_report['recall']['macro avg'] = overall_metrics['recall']\n",
    "formatted_report['f1-score']['macro avg'] = overall_metrics['f1-score']\n",
    "formatted_report['support']['macro avg'] = overall_metrics['support']\n",
    "\n",
    "# Masukkan metrik weighted average\n",
    "weighted_metrics = class_report['weighted avg']\n",
    "formatted_report['precision']['weighted avg'] = weighted_metrics['precision']\n",
    "formatted_report['recall']['weighted avg'] = weighted_metrics['recall']\n",
    "formatted_report['f1-score']['weighted avg'] = weighted_metrics['f1-score']\n",
    "formatted_report['support']['weighted avg'] = weighted_metrics['support']\n",
    "\n",
    "# Tambahkan accuracy\n",
    "formatted_report['accuracy'] = class_report['accuracy']\n",
    "\n",
    "# Ubah ke dalam format JSON\n",
    "import json\n",
    "formatted_report_json = json.dumps(formatted_report, indent=4)\n",
    "\n",
    "# Simpan ke dalam file JSON\n",
    "with open('formatted_classification_report.json', 'w') as json_file:\n",
    "    json_file.write(formatted_report_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d7112d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "output_directory = r'C:\\Users\\iqbal\\OneDrive\\Documents\\BOOTCAMP\\CHALLANGE PLATINUM\\LSTM'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Simpan objek 'onehot' ke dalam file\n",
    "onehot_file_path = os.path.join(output_directory, 'onehot.pkl')\n",
    "with open(onehot_file_path, 'wb') as onehot_file:\n",
    "    pickle.dump(obj=onehot, file=onehot_file)\n",
    "\n",
    "# Simpan objek 'tokenizer' ke dalam file\n",
    "tokenizer_file_path = os.path.join(output_directory, 'tokenizer.pkl')\n",
    "with open(tokenizer_file_path, 'wb') as tokenizer_file:\n",
    "    pickle.dump(obj=tokenizer, file=tokenizer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d233d9d-fcd5-4277-8429-72576b852a03",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iqbal\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('C:/Users/iqbal/OneDrive/Documents/BOOTCAMP/CHALLANGE PLATINUM/LSTM/h5/model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d480a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=train_data_pad.shape[1]#harus diingat untuk padding inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4109a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_lstm=classification_report(y_true=test_data[[\"label\"]],y_pred=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "301a2189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n    negative       0.84      0.85      0.84       660\\n     neutral       0.81      0.77      0.79       202\\n    positive       0.93      0.94      0.94      1338\\n\\n    accuracy                           0.90      2200\\n   macro avg       0.86      0.85      0.86      2200\\nweighted avg       0.89      0.90      0.89      2200\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7517d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = r'C:\\Users\\iqbal\\OneDrive\\Documents\\BOOTCAMP\\CHALLANGE PLATINUM\\LSTM\\json'\n",
    "\n",
    "# Simpan laporan hasil LSTM ke dalam file JSON\n",
    "output_file_path = os.path.join(output_directory, 'report_lstm.json')\n",
    "json.dump(report_lstm, open(output_file_path, \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37db0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
